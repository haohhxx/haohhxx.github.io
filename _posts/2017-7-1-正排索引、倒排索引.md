---
bg: "64495434_p0.jpg"
layout: post
title:  "正排索引、倒排索引"
crawlertitle: "正排索引、倒排索引"
summary: "正排索引、倒排索引"
date:   2017-7-1 1:55:29 +0800
categories: posts
tags: ['java']
author: haohhxx
---

正排索引、倒排索引实现。

```java
package hao.vsm;

import com.google.gson.Gson;
import hao.io.IteratorReader;
import hao.util.WordCut.TokenAnalyzer;
import hao.util.WordCut.TokenFactory;
import jdk.nashorn.internal.parser.TokenStream;

import java.util.*;
import java.util.stream.Stream;

/**
 * Created by haozhenyuan on 2017/4/9.
 *
 */
public class InvertedIndex {

    public static void main(String[] args) {
        InvertedIndex fi = new InvertedIndex();
        TokenAnalyzer toka = TokenFactory.createJiebaSegToken();
        Gson gson = new Gson();
        String path = "E:\\corpus\\聚类\\聚类测试\\id50000.txt.files";
        Iterator<String> ir = new IteratorReader(path);
        int linenub = 0;
        while(ir.hasNext()){
            linenub++;
            if(linenub%1000==0){
                System.out.println(new Date()+"\t"+linenub);
            }
            String line = ir.next();
            Map dataMap = gson.fromJson(line,TreeMap.class);
            String content = (String) dataMap.get("content");
            content = toka.cut(content," ");
            fi.loadLineData(content," ");
        }

        System.out.println(fi.getTfidfMap("新人"));
    }

    private int linenub = -1;
    private TreeMap<String, Map<Integer,Double>> termMap = new TreeMap<>();
    private TreeMap<String, Map<Integer,Double>> termTfidfMap = new TreeMap<>();//docID:TF-IDF
    private Set<String> vocb = new HashSet<String> ();

    public void loadLineData(String line,String regx){
        linenub ++;
        String words[] = line.split(regx);
        Stream.of(words).distinct().forEach(word -> {
            vocb.add(word);
            double tf = (double)Stream.of(words).filter((w) -> {return w.equals(word);}).count();
            double tfsc = tf/(double)words.length;
            Map<Integer,Double> docMap;
            if(termMap.containsKey(word)){
                docMap = termMap.get(word);
            }else{
                docMap = new HashMap<>(); // did : tf
            }
            docMap.put(linenub, tfsc);
            termMap.put(word,docMap);
        });
    }


    public final Map<Integer,Double> getTfidfMap(String term){
        if (termTfidfMap.containsKey(term)) {
            return termTfidfMap.get(term);
        }
        Map<Integer,Double> docMap = termMap.get(term);
        Map<Integer,Double> docTfidfMap = new HashMap<>();
        for(Integer docid:docMap.keySet()){
            double tfsc = docMap.get(docid);
            double tf_idf =  tfsc * Math.log((double)(linenub+1)/(docMap.size()+0.01));
            docTfidfMap.put(docid,tf_idf);
        }
        termTfidfMap.put(term,docTfidfMap);
        return docTfidfMap;
    }


    public final int getDocnub() {
        return linenub;
    }

    public final Set<String> getVocb() {
        return vocb;
    }
}
```


```java

import com.google.gson.Gson;
import com.peopleyuqing.article.Article;
import hao.io.HaoWriter;
import hao.io.IteratorReader;
import hao.util.WordCut.TokenAnalyzer;
import hao.util.WordCut.TokenFactory;

import java.io.Serializable;
import java.util.*;
import java.util.concurrent.*;

/**
 * Created by haozhenyuan on 2017/3/27.
 * 正排索引统计 tf-idf
 */
public class ForwardIndex implements Serializable{

    public static void main(String[] args) throws InterruptedException {
        ForwardIndex fi = new ForwardIndex();
        TokenAnalyzer toka = TokenFactory.createJiebaSegToken();
        Gson gson = new Gson();
        String path = "E:\\corpus\\聚类\\聚类测试\\比对2\\rong\\format\\weixintext.txt.article.txt";
        Iterator<String> ir = new IteratorReader(path);
        int linenub = 0;
        while(ir.hasNext()){
            linenub++;
            if(linenub%1000==0){
                System.out.println(new Date()+"\t"+linenub);
            }
            String line = ir.next();
            Article article = gson.fromJson(line,Article.class);
            String content =  article.getContent();
            content = toka.cut(content," ");
            fi.loadLineData(content);
        }
        fi.getTfidfMap();

        String pathre = "E:\\corpus\\聚类\\聚类测试\\比对2\\rong\\forward\\cosinall.txt";
        HaoWriter hw = new HaoWriter(pathre);
        for(int i = 0;i<linenub;i++){
            if(i%1000==0){
                System.out.println(new Date()+"\t"+i);
            }
            for(int j =(i+1);j<linenub;j++){
                hw.writeLine(i+"\t"+j+"\t"+fi.cosinSim(i,j));
            }
        }
        hw.close();
    }

    private HashMap<String,Integer> termIDMap = new HashMap<>();//TermStr:TermID
    private HashMap<Integer,Integer> docFrequencyMap = new HashMap<>();//TermID:DF
    private TreeMap<Integer, HashMap<Integer,Double>> docTfidfMap = new TreeMap<>();//docID:TF-IDF
    private HashMap<Integer, HashMap<Integer,Double>> indexMap = new HashMap<>();
    private int termID = 0;//词编号
    private int linenub = -1;//句子编号

    public void loadLineData(String line){
        linenub ++;
        HashMap<Integer,Double> termsMap = new HashMap<>(); // tid : tf
        String words[] = line.split(" ");
        HashSet<String> wordsSet = new HashSet<>();
        Collections.addAll(wordsSet,words);
        for(String word:wordsSet){
            double tf = 0; for(String worda:words){ if(word.equals(worda)){tf++;} }//统计 tf
            if(termIDMap.containsKey(word)){
                Integer tid = termIDMap.get(word);
                Integer df = docFrequencyMap.get(tid);
                df++;
                docFrequencyMap.put(tid,df);
                termsMap.put(tid,tf/((double)words.length));
            }else{
                termIDMap.put(word,termID);
                docFrequencyMap.put(termID,1);
                termsMap.put(termID,tf/(double)words.length);
                termID++;
            }
        }
        indexMap.put(linenub,termsMap);
    }

    public int getLineNub(){
        return linenub;
    }
    public HashMap<String, Integer> getTermIDMap() {
        return termIDMap;
    }
    public HashMap<Integer, Integer> getDocFrequencyMap() {
        return docFrequencyMap;
    }
    public HashMap<Integer, HashMap<Integer,Double>> getIndexMap() {
        return indexMap;
    }

    public void getTfidfMap(){
        for(HashMap.Entry<Integer,HashMap<Integer,Double>> ens:indexMap.entrySet()){
            HashMap<Integer,Double> tfidfMap = new HashMap<>();
            Integer key = ens.getKey();
            HashMap<Integer,Double> id_tfMap = ens.getValue();
            for(int id : id_tfMap.keySet()){
                double tf = id_tfMap.get(id);
                double df = docFrequencyMap.get(id);
                double tf_idf = tf * Math.log((double)indexMap.size()/df);
                tfidfMap.put(id,tf_idf);
            }
            docTfidfMap.put(key,tfidfMap);
        }
    }

    public double cosinSim(int i1, int i2){
        HashMap<Integer,Double> map1 = docTfidfMap.get(i1);
        HashMap<Integer,Double> map2 = docTfidfMap.get(i2);

        double numerator=0;
        double denominator1=0;
        double denominator2=0;

        Set<Integer> set1 = map1.keySet();
//        Collections.
        Set<Integer> set2 = map2.keySet();
        Set<Integer> sre = new HashSet<>(set1);
        sre.retainAll(set2);

        if (sre.size() <= 1) {
            return 0D;
        } else {
            for(Integer i:sre){
                numerator += (map1.get(i) * map2.get(i));
            }
            for(double value:map1.values()){
                denominator1 += Math.pow(value,2);
            }
            for(double value:map2.values()){
                denominator2 += Math.pow(value,2);
            }
            double denominator = Math.sqrt(denominator1)*Math.sqrt(denominator2);

            return numerator/denominator;
        }
    }

    /**
     * 写出稀疏的 tf-idf序列
     * @param index doc编号
     * @return tf-idf序列
     */
    public double[] getTfidfArr(int index){
        HashMap<Integer,Double> tfidfMap = docTfidfMap.get(index);
        double tf_idfs[] = new double[termIDMap.size()];
        for(int id : tfidfMap.keySet()){
            double tf_idf = tfidfMap.get(id);
            tf_idfs[id] = tf_idf;
        }
        return tf_idfs;
    }
}

```
